{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5700839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82154\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Fetched 1000 comments.\n",
      "[2/5] Preprocessing done.\n",
      "[3/5] Sentiment analysis done.\n",
      "[5/5] Sentiment counts = {'positive': 289, 'neutral': 587, 'negative': 124}\n",
      "[2/5] Preprocessing done.\n",
      "[3/5] Sentiment analysis done.\n",
      "[5/5] Sentiment counts = {'positive': 289, 'neutral': 587, 'negative': 124}\n",
      "[4a/5] Saved sentiment_dist.png\n",
      "[4b/5] Saved length_dist.png\n",
      "⚠️ No valid timestamps found. Skipping time_series plot.\n",
      "[4a/5] Saved sentiment_dist.png\n",
      "[4b/5] Saved length_dist.png\n",
      "⚠️ No valid timestamps found. Skipping time_series plot.\n",
      "[4d/5] Saved top_keywords.png\n",
      "Top 10 keywords: [('china', 214), ('streams', 202), ('korea', 180), ('speed', 154), ('chinese', 122), ('people', 115), ('world', 93), ('speeds', 84), ('country', 74), ('like', 71)]\n",
      "[4d/5] Saved top_keywords.png\n",
      "Top 10 keywords: [('china', 214), ('streams', 202), ('korea', 180), ('speed', 154), ('chinese', 122), ('people', 115), ('world', 93), ('speeds', 84), ('country', 74), ('like', 71)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_comments(video_id: str, max_comments: int = 1000):\n",
    "    \"\"\"\n",
    "    Step 1: Download up to max_comments from YouTube video and save to raw_comments.json.\n",
    "    P.S: I do NOT pass sort_by_time here, to avoid the TypeError.\n",
    "    \"\"\"\n",
    "    downloader = YoutubeCommentDownloader()\n",
    "    comments = []\n",
    "    \n",
    "    for c in downloader.get_comments(video_id):\n",
    "        comments.append({\n",
    "            \"author\": c[\"author\"],\n",
    "            \"text\": c[\"text\"],\n",
    "            \"time\": c[\"time\"],  \n",
    "        })\n",
    "        if len(comments) >= max_comments:\n",
    "            break\n",
    "    with open(f\"{DATA_DIR}/raw_comments.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(comments, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[1/5] Fetched {len(comments)} comments.\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    return \" \".join(tok for tok in text.split() if tok not in stops)\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\"Step 2: Read raw_comments.json → add clean_text → write clean_comments.json.\"\"\"\n",
    "    with open(f\"{DATA_DIR}/raw_comments.json\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for c in data:\n",
    "        c[\"clean_text\"] = clean_text(c[\"text\"])\n",
    "    with open(f\"{DATA_DIR}/clean_comments.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"[2/5] Preprocessing done.\")\n",
    "\n",
    "\n",
    "def analyze_sentiment():\n",
    "    \"\"\"Step 3: Read clean_comments.json → VADER analysis → write sentiment_comments.json.\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    with open(f\"{DATA_DIR}/clean_comments.json\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for c in data:\n",
    "        c[\"sentiment\"] = analyzer.polarity_scores(c[\"clean_text\"])\n",
    "    with open(f\"{DATA_DIR}/sentiment_comments.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"[3/5] Sentiment analysis done.\")\n",
    "\n",
    "def load_data():\n",
    "    with open(f\"{DATA_DIR}/sentiment_comments.json\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def sentiment_distribution(data):\n",
    "    dist = Counter()\n",
    "    for c in data:\n",
    "        comp = c[\"sentiment\"][\"compound\"]\n",
    "        if comp >= 0.05:      dist[\"positive\"] += 1\n",
    "        elif comp <= -0.05:   dist[\"negative\"] += 1\n",
    "        else:                 dist[\"neutral\"]  += 1\n",
    "    return dist\n",
    "\n",
    "def plot_sentiment(dist):\n",
    "    labels, counts = zip(*dist.items())\n",
    "    plt.figure()\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/sentiment_dist.png\")\n",
    "    plt.close()\n",
    "    print(\"[4a/5] Saved sentiment_dist.png\")\n",
    "\n",
    "def length_distribution(data):\n",
    "    lengths = [len(c[\"clean_text\"].split()) for c in data]\n",
    "    plt.figure()\n",
    "    plt.hist(lengths, bins=20)\n",
    "    plt.title(\"Comment Length Distribution\")\n",
    "    plt.xlabel(\"Number of Tokens\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/length_dist.png\")\n",
    "    plt.close()\n",
    "    print(\"[4b/5] Saved length_dist.png\")\n",
    "\n",
    "\n",
    "def time_series(data):\n",
    "    \"\"\"\n",
    "    Step 4: Try to create a time series plot of comment counts per day.\n",
    "    \"\"\"\n",
    "    valid_dates = []\n",
    "    for c in data:\n",
    "        try:\n",
    "            # Convert milliseconds to seconds, then to datetime.date\n",
    "            ts = int(c[\"time\"]) / 1000\n",
    "            date = datetime.datetime.fromtimestamp(ts).date()\n",
    "            valid_dates.append(date)\n",
    "        except (KeyError, TypeError, ValueError):\n",
    "            continue  \n",
    "\n",
    "    if not valid_dates:\n",
    "        print(\"⚠️ No valid timestamps found. Skipping time_series plot.\")\n",
    "        return\n",
    "\n",
    "    # Count comments per date\n",
    "    date_counts = Counter(valid_dates)\n",
    "    sorted_dates = sorted(date_counts.items())\n",
    "\n",
    "    x, y = zip(*sorted_dates)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, y, marker='o', linestyle='-')\n",
    "    plt.title(\"Comments Over Time\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Comments\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/time_series.png\")\n",
    "    plt.close()\n",
    "    print(\"[4c/5] Saved time_series.png\")\n",
    "\n",
    "def plot_top_keywords(data, top_n=10):\n",
    "    \"\"\"\n",
    "    Step 4d/5: Plot top N keywords from clean_text.\n",
    "    \"\"\"\n",
    "    # 1. Gather all tokens\n",
    "    tokens = []\n",
    "    for c in data:\n",
    "        tokens.extend(c[\"clean_text\"].split())\n",
    "    # 2. Get the top N most common\n",
    "    freq = Counter(tokens).most_common(top_n)\n",
    "    words, counts = zip(*freq)\n",
    "\n",
    "    # 3. Plot & save\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(f\"Top {top_n} Keywords in Comments\")\n",
    "    plt.xlabel(\"Keyword\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/top_keywords.png\")\n",
    "    plt.close()\n",
    "    print(\"[4d/5] Saved top_keywords.png\")\n",
    "\n",
    "def top_keywords(data, top_n=10):\n",
    "    tokens = []\n",
    "    for c in data:\n",
    "        tokens.extend(c[\"clean_text\"].split())\n",
    "    return Counter(tokens).most_common(top_n)\n",
    "\n",
    "def main():\n",
    "    video_id = \"fK85SQzm0Z0\"\n",
    "    fetch_comments(video_id)\n",
    "    preprocess()\n",
    "    analyze_sentiment()\n",
    "\n",
    "    data = load_data()\n",
    "    dist = sentiment_distribution(data)\n",
    "    print(f\"[5/5] Sentiment counts = {dict(dist)}\")\n",
    "\n",
    "    plot_sentiment(dist)\n",
    "    length_distribution(data)\n",
    "    time_series(data)\n",
    "    plot_top_keywords(data)         \n",
    "\n",
    "    top10 = top_keywords(data)\n",
    "    print(\"Top 10 keywords:\", top10)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa9bd9",
   "metadata": {},
   "source": [
    "# Emotion Analysis of YouTube Comments\n",
    "This section analyzes the emotions present in the comments using the NRC Emotion Lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e570ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NRC Emotion Lexicon if not present\n",
    "import pandas as pd\n",
    "import requests\n",
    "NRC_URL = \"https://raw.githubusercontent.com/words/lexicon/master/nrc/nrc.txt\"\n",
    "NRC_PATH = os.path.join(DATA_DIR, \"nrc_lexicon.txt\")\n",
    "if not os.path.exists(NRC_PATH):\n",
    "    try:\n",
    "        r = requests.get(NRC_URL)\n",
    "        with open(NRC_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "    except Exception as e:\n",
    "        print(\"Could not download NRC lexicon:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92cb350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NRC Lexicon\n",
    "def load_nrc_lexicon():\n",
    "    lexicon = {}\n",
    "    with open(NRC_PATH, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, emotion, value = line.strip().split(\"\\t\")\n",
    "            if int(value) == 1:\n",
    "                if word not in lexicon:\n",
    "                    lexicon[word] = set()\n",
    "                lexicon[word].add(emotion)\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f80207f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze emotions in comments\n",
    "def analyze_emotions(data):\n",
    "    lexicon = load_nrc_lexicon()\n",
    "    emotions = [\n",
    "        \"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"\n",
    "    ]\n",
    "    emotion_counter = Counter({e: 0 for e in emotions})\n",
    "    for c in data:\n",
    "        tokens = c[\"clean_text\"].split()\n",
    "        for tok in tokens:\n",
    "            if tok in lexicon:\n",
    "                for emo in lexicon[tok]:\n",
    "                    if emo in emotion_counter:\n",
    "                        emotion_counter[emo] += 1\n",
    "    return emotion_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11c1ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot emotion distribution\n",
    "def plot_emotions(emotion_counts):\n",
    "    emotions, counts = zip(*emotion_counts.items())\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(emotions, counts, color=\"purple\")\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count}', ha='center', va='bottom', fontsize=9, color='blue')\n",
    "    plt.title(\"Emotion Distribution in Comments\")\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DATA_DIR}/emotion_dist.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved emotion_dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a6167a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Run emotion analysis after loading data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m----> 3\u001b[0m emotion_counts \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotion counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, emotion_counts)\n\u001b[0;32m      5\u001b[0m plot_emotions(emotion_counts)\n",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m, in \u001b[0;36manalyze_emotions\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_emotions\u001b[39m(data):\n\u001b[1;32m----> 3\u001b[0m     lexicon \u001b[38;5;241m=\u001b[39m \u001b[43mload_nrc_lexicon\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     emotions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manger\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manticipation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisgust\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msadness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurprise\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m     emotion_counter \u001b[38;5;241m=\u001b[39m Counter({e: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m emotions})\n",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m, in \u001b[0;36mload_nrc_lexicon\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(NRC_PATH, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m----> 6\u001b[0m         word, emotion, value \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lexicon:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "# Example: Run emotion analysis after loading data\n",
    "data = load_data()\n",
    "emotion_counts = analyze_emotions(data)\n",
    "print(\"Emotion counts:\", emotion_counts)\n",
    "plot_emotions(emotion_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b248fb",
   "metadata": {},
   "source": [
    "# Topic Modeling of YouTube Comments\n",
    "Discover the main discussion topics in the comments using Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "869a515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim import corpora\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install gensim\n",
    "    import gensim\n",
    "    from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "add6f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling with LDA\n",
    "def run_lda_topic_modeling(data, num_topics=5, num_words=7):\n",
    "    # Prepare documents\n",
    "    documents = [c['clean_text'].split() for c in data]\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    # Train LDA model\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
    "    topics = lda_model.print_topics(num_words=num_words)\n",
    "    for topic in topics:\n",
    "        print(f'Topic: {topic}')\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1b2dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LDA topics as a bar chart\n",
    "def plot_lda_topics(topics):\n",
    "    topic_labels = []\n",
    "    topic_words = []\n",
    "    for idx, topic in enumerate(topics):\n",
    "        topic_str = topic[1] if isinstance(topic, tuple) else topic\n",
    "        # Extract words from topic string\n",
    "        words = [w.split('*')[1].replace('\"','').strip() for w in topic_str.split(' + ')]\n",
    "        topic_labels.append(f'Topic {idx+1}')\n",
    "        topic_words.append(', '.join(words))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(topic_labels, [1]*len(topic_labels), color='teal')\n",
    "    for bar, words in zip(bars, topic_words):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), words, ha='center', va='bottom', fontsize=10, color='blue', rotation=45)\n",
    "    plt.title('LDA Topics (Top Words per Topic)')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Relative Importance (for display only)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{DATA_DIR}/lda_topics.png')\n",
    "    plt.close()\n",
    "    print('Saved lda_topics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8504729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: (0, '0.035*\"streams\" + 0.033*\"korea\" + 0.014*\"speeds\" + 0.013*\"koreans\" + 0.013*\"chinese\" + 0.013*\"world\" + 0.011*\"irl\"')\n",
      "Topic: (1, '0.030*\"santai\" + 0.028*\"china\" + 0.020*\"speed\" + 0.013*\"w\" + 0.012*\"come\" + 0.010*\"like\" + 0.007*\"stream\"')\n",
      "Topic: (2, '0.017*\"china\" + 0.015*\"speed\" + 0.014*\"people\" + 0.009*\"bro\" + 0.007*\"chinese\" + 0.007*\"even\" + 0.006*\"love\"')\n",
      "Topic: (3, '0.019*\"china\" + 0.013*\"jackson\" + 0.010*\"wang\" + 0.009*\"speed\" + 0.008*\"people\" + 0.006*\"girl\" + 0.005*\"bro\"')\n",
      "Topic: (4, '0.032*\"china\" + 0.030*\"speed\" + 0.011*\"dont\" + 0.010*\"chinese\" + 0.007*\"im\" + 0.006*\"ishowspeed\" + 0.006*\"bro\"')\n",
      "Saved lda_topics.png\n"
     ]
    }
   ],
   "source": [
    "# Example: Run topic modeling and plot topics after loading data\n",
    "data = load_data()\n",
    "topics = run_lda_topic_modeling(data, num_topics=5, num_words=7)\n",
    "plot_lda_topics(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "748be1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LDA topics graph as HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.backends.backend_svg import FigureCanvasSVG\n",
    "def save_lda_topics_html(topics):\n",
    "    import base64\n",
    "    import io\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    topic_labels = []\n",
    "    topic_words = []\n",
    "    for idx, topic in enumerate(topics):\n",
    "        topic_str = topic[1] if isinstance(topic, tuple) else topic\n",
    "        words = [w.split('*')[1].replace('\"','').strip() for w in topic_str.split(' + ')]\n",
    "        topic_labels.append(f'Topic {idx+1}')\n",
    "        topic_words.append(', '.join(words))\n",
    "    bars = ax.bar(topic_labels, [1]*len(topic_labels), color='teal')\n",
    "    for bar, words in zip(bars, topic_words):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), words, ha='center', va='bottom', fontsize=10, color='blue', rotation=45)\n",
    "    ax.set_title('LDA Topics (Top Words per Topic)')\n",
    "    ax.set_xlabel('Topic')\n",
    "    ax.set_ylabel('Relative Importance (for display only)')\n",
    "    fig.tight_layout()\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    html = f'''<html><head><title>LDA Topics</title></head><body><h2>LDA Topics (Top Words per Topic)</h2><img src=\"data:image/png;base64,{img_base64}\"/></body></html>'''\n",
    "    with open(f'{DATA_DIR}/lda_topics.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    plt.close(fig)\n",
    "    print('Saved lda_topics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "01904561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: (0, '0.035*\"streams\" + 0.033*\"korea\" + 0.014*\"speeds\" + 0.013*\"koreans\" + 0.013*\"chinese\" + 0.013*\"world\" + 0.011*\"irl\"')\n",
      "Topic: (1, '0.030*\"santai\" + 0.028*\"china\" + 0.020*\"speed\" + 0.013*\"w\" + 0.012*\"come\" + 0.010*\"like\" + 0.007*\"stream\"')\n",
      "Topic: (2, '0.017*\"china\" + 0.015*\"speed\" + 0.014*\"people\" + 0.009*\"bro\" + 0.007*\"chinese\" + 0.007*\"even\" + 0.006*\"love\"')\n",
      "Topic: (3, '0.019*\"china\" + 0.013*\"jackson\" + 0.010*\"wang\" + 0.009*\"speed\" + 0.008*\"people\" + 0.006*\"girl\" + 0.005*\"bro\"')\n",
      "Topic: (4, '0.032*\"china\" + 0.030*\"speed\" + 0.011*\"dont\" + 0.010*\"chinese\" + 0.007*\"im\" + 0.006*\"ishowspeed\" + 0.006*\"bro\"')\n",
      "Saved lda_topics.html\n"
     ]
    }
   ],
   "source": [
    "# Example: Run topic modeling and save HTML after loading data\n",
    "data = load_data()\n",
    "topics = run_lda_topic_modeling(data, num_topics=5, num_words=7)\n",
    "save_lda_topics_html(topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
